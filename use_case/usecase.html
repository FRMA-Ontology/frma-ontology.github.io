<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=7ebMze0MJP7bZu91CQ7uEApNHmxKgM3Y09dQPiEfh2Y');ol.lst-kix_list_1-3{list-style-type:none}ol.lst-kix_list_1-4{list-style-type:none}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_list_1-1{list-style-type:none}ol.lst-kix_list_1-2{list-style-type:none}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ". "}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,lower-latin) ". "}.lst-kix_list_1-2>li:before{content:"" counter(lst-ctn-kix_list_1-2,lower-roman) ". "}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_1-8{list-style-type:none}.lst-kix_list_1-3>li:before{content:"" counter(lst-ctn-kix_list_1-3,decimal) ". "}.lst-kix_list_1-4>li:before{content:"" counter(lst-ctn-kix_list_1-4,lower-latin) ". "}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_list_1-7>li:before{content:"" counter(lst-ctn-kix_list_1-7,lower-latin) ". "}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:"" counter(lst-ctn-kix_list_1-5,lower-roman) ". "}.lst-kix_list_1-6>li:before{content:"" counter(lst-ctn-kix_list_1-6,decimal) ". "}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}.lst-kix_list_1-8>li:before{content:"" counter(lst-ctn-kix_list_1-8,lower-roman) ". "}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}ol{margin:0;padding:0}table td,table th{padding:0}.c27{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:121.5pt;border-top-color:#000000;border-bottom-style:solid}.c107{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c66{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#000000;border-bottom-style:solid}.c17{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c80{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#000000;border-bottom-style:solid}.c76{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:391.1pt;border-top-color:#4f81bd;border-bottom-style:solid}.c64{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.8pt;border-top-color:#4f81bd;border-bottom-style:solid}.c63{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:116.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c22{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c93{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#000000;border-bottom-style:solid}.c23{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c97{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#000000;border-bottom-style:solid}.c94{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#4f81bd;border-bottom-style:solid}.c44{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.5pt;border-top-color:#000000;border-bottom-style:solid}.c81{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:467.1pt;border-top-color:#4f81bd;border-bottom-style:solid}.c99{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:463.5pt;border-top-color:#000000;border-bottom-style:solid}.c100{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:240.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c30{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:97.5pt;border-top-color:#000000;border-bottom-style:solid}.c43{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#000000;border-bottom-style:solid}.c103{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#000000;border-bottom-style:solid}.c32{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:53.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c42{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c87{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.8pt;border-top-color:#000000;border-bottom-style:solid}.c18{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:121.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c10{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:240.3pt;border-top-color:#000000;border-bottom-style:solid}.c71{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:54.2pt;border-top-color:#000000;border-bottom-style:solid}.c82{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:389.4pt;border-top-color:#4f81bd;border-bottom-style:solid}.c13{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:95.2pt;border-top-color:#000000;border-bottom-style:solid}.c72{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c102{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#000000;border-bottom-style:solid}.c14{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:389.4pt;border-top-color:#4f81bd;border-bottom-style:solid}.c49{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:31.5pt;border-top-color:#000000;border-bottom-style:solid}.c59{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:53.5pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c53{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c68{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#000000;border-bottom-style:solid}.c90{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#4f81bd;border-bottom-style:solid}.c58{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:274.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c50{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#4f81bd;border-bottom-style:solid}.c105{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#4f81bd;border-bottom-style:solid}.c86{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c8{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:238.5pt;border-top-color:#000000;border-bottom-style:solid}.c1{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:77.7pt;border-top-color:#4f81bd;border-bottom-style:solid}.c62{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:97.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c19{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#4f81bd;border-bottom-style:solid}.c73{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#000000;border-bottom-style:solid}.c75{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c89{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c38{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#000000;border-bottom-style:solid}.c52{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:54.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c85{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c77{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:274.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c91{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:467.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c79{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c65{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c55{padding-top:0pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c95{-webkit-text-decoration-skip:none;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Times New Roman";font-style:normal}.c101{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:right}.c11{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none;font-size:10pt;font-style:italic}.c33{-webkit-text-decoration-skip:none;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:10pt;font-family:"Times"}.c56{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c46{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c15{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c12{font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Times";font-style:normal}.c6{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;height:12pt}.c83{text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c104{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c45{margin-left:-5.8pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c39{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman"}.c57{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c40{color:#0563c1;vertical-align:baseline;font-style:normal}.c106{color:#000000;vertical-align:baseline;font-style:normal}.c88{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c48{font-size:9pt;color:#ffffff;font-weight:700}.c25{color:#000000;font-size:12pt}.c92{margin-left:72pt;text-indent:-72pt}.c2{color:#000000;font-size:10pt}.c70{margin-left:40.5pt;text-indent:-40.5pt}.c3{color:inherit;text-decoration:inherit}.c34{font-family:"Times";font-weight:400}.c36{color:#ff0000;font-size:10pt}.c21{font-size:10pt;font-style:italic}.c54{color:#ffffff;font-weight:700}.c96{font-family:"Ubuntu";font-weight:400}.c84{color:#ff0000;font-size:12pt}.c78{padding:0;margin:0}.c26{color:#000000;font-weight:700}.c69{color:#000000;font-size:8pt}.c51{margin-left:15.2pt;padding-left:0pt}.c24{font-style:italic}.c67{font-family:"Times"}.c7{height:0pt}.c37{font-size:10pt}.c41{color:#0563c1}.c29{height:15pt}.c98{height:11pt}.c35{height:3pt}.c47{height:13pt}.c28{background-color:#4f81bd}.c61{height:27pt}.c9{height:12pt}.c74{height:14pt}.c31{height:51pt}.c60{font-weight:700}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c88"><div><p class="c101"><span class="c15 c69">Team FRMA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew Johnson // Alexander Schwartzberg // John Angel // Jordan Faas-Bush // Andrew Gaudet</span></p></div><p class="c9 c104" id="h.gjdgxs"><span class="c65"></span></p><a id="t.a2a4967e6fd70144df120c4bffab96be507e333d"></a><a id="t.0"></a><table class="c45"><tbody><tr class="c47"><td class="c28 c76" colspan="2" rowspan="1"><p class="c0"><span class="c54">I. Use Case Description</span></p></td></tr><tr class="c98"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Use Case Name</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0"><span class="c2 c24">Facial Recognition Model Analyzer</span></p></td></tr><tr class="c74"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Use Case Identifier</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0"><span class="c4">1</span></p></td></tr><tr class="c47"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Source</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0 c9"><span class="c4"></span></p></td></tr><tr class="c98"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Point of Contact</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0"><span class="c2 c24">Matthew Johnson, </span><span class="c11"><a class="c3" href="mailto:johnsm21@rpi.edu">johnsm21@rpi.edu</a></span></p><p class="c0"><span class="c21">Alexander Schwartzberg, </span><span class="c11"><a class="c3" href="mailto:schwaa6@rpi.edu">schwaa6@rpi.edu</a></span></p><p class="c0"><span class="c21">John Angel, </span><span class="c11"><a class="c3" href="mailto:angelj3@rpi.edu">angelj3@rpi.edu</a></span><span class="c39 c2 c24">&nbsp;</span></p><p class="c0"><span class="c21">Jordan Faas-Bush, </span><span class="c11"><a class="c3" href="mailto:faasbj@rpi.edu">faasbj@rpi.edu</a></span></p><p class="c0"><span class="c21">Andrew Gaudet, </span><span class="c11"><a class="c3" href="mailto:gaudea@rpi.edu">gaudea@rpi.edu</a></span></p></td></tr><tr class="c98"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Creation / Revision Date</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0"><span class="c2">September 22, 2018</span></p></td></tr><tr class="c7"><td class="c63" colspan="1" rowspan="1"><p class="c0"><span class="c2">Associated Documents</span></p></td><td class="c58" colspan="1" rowspan="1"><p class="c0"><span class="c15 c2">https://tw.rpi.edu/web/Courses/Ontologies/2018/FRMA</span></p></td></tr></tbody></table><p class="c0 c9"><span class="c4"></span></p><a id="t.189df6ac3407bda99031ee93256084226e7700de"></a><a id="t.1"></a><table class="c45"><tbody><tr class="c7"><td class="c28 c81" colspan="2" rowspan="1"><p class="c0"><span class="c54">II. Use Case Summary</span></p></td></tr><tr class="c29"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Goal</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Machine learning allows us to learn a model for a given task such as facial recognition with a high degree of accuracy. However, after these models are generated they are often treated as black boxes and the limitations of a model are often unknown to the end user. The system developed for this use case will provide an intuitive interface to explore the limits of a facial recognition model by semantically integrating &ldquo;smart&rdquo; images(semantically describe who the image depicts and what Kumar [2] features the image exhibits) with classification results to discover common causes for misclassifications.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Requirements</span></p></td><td class="c14" colspan="1" rowspan="1"><ol class="c78 lst-kix_list_1-0 start" start="1"><li class="c0 c51"><span class="c2 c34">Need an ontology describing the hierarchy of image features from Kumar et al. paper [3] (the ontology will be designed and implemented as part of this project)</span><span class="c34 c37">. The ontology will be designed and implemented as part of this project.</span></li><li class="c0 c51"><span class="c2 c34">Needs to able load test results for a given model and link the results to images from the </span><span class="c34 c37">Labeled Faces in the Wild (</span><span class="c2 c34">LF</span><span class="c34 c37">W)</span><span class="c6 c2">&nbsp;dataset [1]</span></li></ol></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Scope</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">The scope for this use case is limited to the LFW dataset, the LFW ground truth [1] and a single facial recognition model FaceNet [2]. In addition, we will only consider image attributes learned in Kumar et al. paper [3]</span><span class="c2 c34">.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Priority</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c2 c24">Identify the priority of the use case (with respect to other use cases for the project)</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Stakeholders</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c33">Machine Learning Researchers</span><span class="c6 c2">- Did your new methodology develop a model that covers the training domain specifically or can it be easily transferred to new domains?</span></p><p class="c0"><span class="c33">Application Engineer or Program Manager</span><span class="c34 c37">- You have ten different machine learning models you could include in your product and no test data; how do you choose which to include?</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Description</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c2 c34">The use case is focused on requirements for the development of an application to support fac</span><span class="c34 c37">ial</span><span class="c2 c34">&nbsp;recognition model analysis. It will be used to discover how an ontology of image attributes correlate to the test results of a fac</span><span class="c34 c37">ial</span><span class="c6 c2">&nbsp;recognition model. The application must be able to read in model test results provided by a user and link classification results to known &ldquo;smart&rdquo; images which have a hierarchical ontology of visual features describing the image content. Using this alignment between datasets the system will be able to calculate accuracy statistics across images attributes and find correlations between misclassifications and image attributes.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c2 c34">The system will present this information as a semantically enhanced hierarchy view of attributes with the statistics of all images that contain that attribute (i</span><span class="c34 c37">.e &lsquo;</span><span class="c2 c34">has goatee</span><span class="c34 c37">&rsquo;</span><span class="c2 c34">) or belong under an attribute category (i.e. </span><span class="c34 c37">&lsquo;</span><span class="c2 c34">has facial hair</span><span class="c34 c37">&rsquo;</span><span class="c2 c34">). When a user selects a level on this category they will be presented with a view showing the images associated with that category and an indication of whether they were classified correctly or not. In addition, the user should be able to switch between all images, correctly classified images, and misclassifications which cause the hierarchy view to update statistics and the images presented.</span><span class="c2">&nbsp;</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Actors / Interfaces</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer (person who wants to know more about a model)</span></p><p class="c0"><span class="c6 c2">Labeled Faces in the Wild Dataset [1]</span></p><p class="c0"><span class="c6 c2">The test results of the model being evaluated.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Pre-conditions</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The system must have the image attributes lifted into RDF and aligned with the feature ontology.</span></p><p class="c0"><span class="c6 c2">The model results that the user is inputting into the system must have used the LFW dataset as their training dataset.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Post-conditions</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Postconditions (Success) - The user loads their model&rsquo;s test results into the Model Analyzer to generate a hierarchical view of misclassifications and notices that model misclassified facial hair the most and examines the various misclassified photos.</span></p><p class="c0 c9 c92"><span class="c6 c2"></span></p><p class="c0"><span class="c34 c37">Postconditions</span><span class="c37 c60 c67">&nbsp;</span><span class="c6 c2">(Failure) - The user tries to load their model&rsquo;s test results into the Model Analyzer, but the file format was incorrect. As a result, the user was unable to generate a hierarchical view.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Triggers</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">A user submits the test results for a model and selects the model from which the results were generated, or a user selects a previously uploaded test result set.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Performance Requirements</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c2 c24">List any known performance-specific requirements &ndash; timing and sizing (volume, frequency, etc.), maintainability, reusability, other &ldquo;-ilities&rdquo;, etc.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Assumptions </span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0"><span class="c37">Image attributes will correlate with the performance of the model being examined. Past experiments have shown that image factors such as quality and facial occlusion reduce the accuracy of facial recognition [5], but there&rsquo;s no guarantee the facenet classification results will reflect that.</span></p></td></tr><tr class="c7"><td class="c1" colspan="1" rowspan="1"><p class="c0"><span class="c2">Open Issues</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c0 c9"><span class="c4"></span></p></td></tr></tbody></table><p class="c46 c9"><span class="c4"></span></p><p class="c0"><span class="c26">III. Usage Scenarios</span></p><p class="c0 c9"><span class="c36 c24 c39"></span></p><p class="c0"><span class="c2 c34">A user wants to learn the limitations of their new fa</span><span class="c34 c37">cial</span><span class="c2 c34">&nbsp;recognition model, so they run the model against a test set and capture the classification result of each picture in a </span><span class="c34 c37">CSV </span><span class="c2 c34">file. The user th</span><span class="c34 c37">e</span><span class="c2 c34">n takes this file and imports it into the Model Analyzer web portal and chooses the dataset the model was run against. The system then lifts the results into RDF and links each test result to a &ldquo;smart&rdquo; image which has the image attributes described in an ontology. The system then generates a </span><span class="c6 c2">semantically enhanced hierarchy view of attributes with the classification accuracy of images that contain that attribute. The user then switches the system from &lsquo;all&rsquo; to &lsquo;misclassification only&rsquo; and the hierarchy view updates to show attribute misclassification accuracy. From this view the user notices that the model misclassifies images with facial hair the most and clicks on this level of the hierarchy. The portal than populates the screen with the test images from that attribute level that were misclassified, allowing further investigation.</span></p><p class="c0 c9"><span class="c39 c36 c24"></span></p><p class="c0"><span class="c2 c34">A user wants to know if </span><span class="c34 c37">their </span><span class="c2 c34">previously analyzed </span><span class="c34 c37">facial recognition</span><span class="c2 c34">&nbsp;model would do well on images with poor lighting. The user opens the Model Analyzer web portal and chooses a previously run data model from the list. The system then loads in the previous results into RDF and links each test result to a &ldquo;smart&rdquo; image which has the image attributes described in an ontology. The system then generates a </span><span class="c6 c2">semantically enhanced hierarchy view of attributes with counts of the number of images that contain that attribute. From this view the user selects both the harsh lighting and soft lighting image attributes. The portal then populates the screen with the test images that contain either of those attributes and a combined accuracy. The user leverages this accuracy figure to conclude that their old model will work well in poor lighting conditions.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c55"><span class="c25 c60 c83">IV. Basic Flow of Events</span></p><p class="c55 c9"><span class="c4"></span></p><a id="t.d26ecc0077df9c0c654710edfc8a43fff7731452"></a><a id="t.2"></a><table class="c45"><tbody><tr class="c7"><td class="c28 c91" colspan="4" rowspan="1"><p class="c0"><span class="c54">Basic / Normal Flow of Events</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Step</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Actor (Person)</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Actor (System)</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Description</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">1</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User accesses Model Analyzer portal</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">2</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">User selects model used to generate test results </span><span class="c12 c2">(FaceNet, dlib)</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">2</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User uploads new model test results</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">3</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User selects corresponding test dataset from list.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">4</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Portal retrieves data from attribute ontology and integrates model test results</span></p></td></tr><tr class="c35"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">5</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user&rsquo;s browser renders a semantically enhanced hierarchy view of attributes with counts of number of images with that attribute. The system will use the ground truth data in the LFW Attributes dataset to associate images used in the user&rsquo;s test results to Kumar[2] attributes. Then it would simply count the occurrences of attributes in the users test results. Additional note: the &ldquo;hierarchy view&rdquo; structure is directly taken from the attribute ontology that we will be building.</span></p></td></tr><tr class="c35"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">6</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User selects misclassifications from a dropdown menu</span></p></td></tr><tr class="c35"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">7</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user&rsquo;s browser updates the hierarchy so the counts show the misclassification on each attribute level. The system accomplishes this by first accessing the ground truths within the LFW dataset and comparing them to the inputted test results to obtain which images were correctly identified in the test results. This information refined by an inference rule that compares the TestResult with the ground truth and assert if the model classified the image correctly.. The system will then take this information and combine it with the LFW Attributes dataset&rsquo;s ground truths to associate those test results to image attributes. From here only results that were misclassified will be shown.</span></p></td></tr><tr class="c35"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">8</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user selects some level on the hierarchy.</span></p></td></tr><tr class="c35"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">9</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c30" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c10" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The portal shows all images with that attribute level that were misclassified. This will be accomplished through similar logic to step 7 with the exception that only results of a certain level&rsquo;s class will be show.</span></p></td></tr></tbody></table><p class="c0 c9"><span class="c4"></span></p><p class="c55"><span class="c26">V. Alternate Flow of Events</span></p><p class="c0 c9"><span class="c4"></span></p><a id="t.217f838a927f861d18289d93655b9ff0bb4db90c"></a><a id="t.3"></a><table class="c45"><tbody><tr class="c7"><td class="c28 c99" colspan="4" rowspan="1"><p class="c0"><span class="c54">Alternate Flow of Events</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Step</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Actor (Person)</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Actor (System)</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c2 c60">Description</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">1</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User accesses Model Analyzer portal.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">2</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">User selects model used to generate test results </span><span class="c12 c2">(FaceNet, dlib)</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">2</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c12 c2">User selects previously uploaded model test results.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">3</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User selects corresponding test dataset from list.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">4</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Portal retrieves data from attribute ontology and integrates model test results.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">5</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user&rsquo;s browser renders semantically enhanced hierarchy view of attributes, which displays a tree view showing top level image characteristic such as lighting variation, depicted person attributes, along with mid-level attributes such as wearables, and low level attributes like hats. An example tree is shown in the notes section below. Across each level of this tree we will collect counts of number of images with that attribute. The system will use the ground truth data in the LFW Attributes dataset to associate images used in the user&rsquo;s test results to Kumar attributes. Then it would simply count the occurrences of attributes in the users test results. Additional note: the &ldquo;hierarchy view&rdquo; structure is directly taken from the attribute ontology that we will be building.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">6</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">User selects misclassifications from a dropdown menu</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">7</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user&rsquo;s browser updates the hierarchy so the counts show the misclassification on each attribute level. The system accomplishes this by first accessing the ground truths within the LFD dataset and comparing them to the inputted test results to obtain which images were correctly identified in the test results. This information refined by some inference rules will give which images were misclassified. For example if we get a test result that says image 203 is Steve Irwin, we can perform a query to get the ground truth data on image 203 and if that image depicts Steve Irwin, we will assert a new triple saying that the classification result is correct.The system will then take this information and combine it with the LFW Attributes dataset&rsquo;s ground truths to associate those test results to image attributes. From here only results that were misclassified will be shown.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">8</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Researcher/Engineer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The user selects some level on the hierarchy.</span></p></td></tr><tr class="c7"><td class="c49" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">9</span></p></td><td class="c38" colspan="1" rowspan="1"><p class="c0 c9"><span class="c2 c6"></span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Model Analyzer App</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">The portal shows all images with that attribute level that were misclassified. This will be accomplished through similar logic to &nbsp;Step 7 with the exception that only results of a certain level&rsquo;s class will be shown. An example of what the hierarchy view will look like is shown in the notes section.</span></p><p class="c0 c9"><span class="c6 c2"></span></p></td></tr></tbody></table><hr style="page-break-before:always;display:none;"><p class="c46 c9"><span class="c4"></span></p><p class="c0"><span class="c83 c25 c60">VI. Use Case and Activity Diagram(s)</span></p><p class="c56 c9"><span class="c4"></span></p><p class="c56"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.57px; height: 612.50px;"><img alt="" src="images/image1.png" style="width: 439.57px; height: 612.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c56"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 358.67px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 358.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c56"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 551.00px; height: 446.48px;"><img alt="" src="images/image2.png" style="width: 551.00px; height: 446.48px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c26">VII. Competency Questions</span></p><p class="c0 c9"><span class="c4"></span></p><p class="c0"><span class="c12 c2">Q: What type of image attributes does the model have the most trouble classifying?</span></p><p class="c0"><span class="c6 c2">A: 45% of misclassification occurs on images that contain facial hair.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c34 c37">Prior to the triggers the system will consist of several components: LFW[1] smart images which semantically describe who the image depicts and what Kumar [2] features the image in a similar to Image Snippet[6] and a visual concept ontology that describes a taxonomy of Kumar [2] features. Model test results will be loaded into the system and will be lifted to RDF. The system will then use an inference rule and the smart images to determine if an images within the test result are classified correctly or misclassified. For example if we get a test result that says image 203 is Steve Irwin, we can perform a query to get the ground truth data on image 203 and if that image depicts Steve Irwin, we will assert a new triple saying that the classification result is correct.Another inference rule will calculate the accuracy of classified images with an attribute by counting the number of correctly classified images with the attribute and dividing by all images with the attribute. This process will be repeated for each attribute and upper level term within the ontology of visual concepts to calculate accuracy. Afterwards another inferencing rule will examine the accuracies and provide a rating of great, good, ok, fair, bad. Finally we will display these results to the end user through the hierarchical view that shows the hierarchy of attributes/upper terms and the accuracy, ratings. The hierarchy view will start by displaying the results for the highest levels of the taxonomy tree and show every image and the accuracy overall, but the user will be able to select a lower level of the tree such as whether the photo used a flash, whether the subject is smiling or not, or, as listed in this competency question, whether or not the subject has facial hair, and the view will only show the results for photos that belong to that section of the hierarchy tree.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c12 c2">Q: What type of facial attributes does the model most associate with George W. Bush?</span></p><p class="c0"><span class="c6 c2">A: &lsquo;White&rdquo;, &ldquo;appears masculine&rdquo;, and &ldquo;wearing tie&rdquo; are most associated with George W. Bush.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c6 c2">Since the model test results are known, we can link them to our &ldquo;smart&rdquo; images that contain attribute tagging. From this we can look at all the images of George W. Bush that were classified properly using the LFW ground truth and count which attributes occur more frequently by running a SPARQL query counting the number of occurrences of each attribute in images that were classified as &quot;George W. Bush&quot; by the user&rsquo;s model - for instance, &ldquo;wearing necktie&rdquo;. Then, using our ontology&rsquo;s taxonomy structure for attributes, we can determine that this attribute is a type of &ldquo;wearable.&rdquo; We then display the results of these queries using the hierarchy view as mentioned before, only this time instead of showing the accuracy statistics and percentages we show the accumulated number of images that the model has classified as George W. Bush and contains one or more of the associated attributes (or sub attributes). The user is then able to look at the hierarchy to determine which attributes are most associated with George W. Bush at the level of detail that they are interested in. This query could work for any attribute we (using the Kumar features) or the model have tagged, be it the person&rsquo;s name or an image attribute.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c12 c2">Q: Which of these two models is better at classifying people with long hair?</span></p><p class="c0"><span class="c6 c2">A: FaceNet is 10% better than dlib at classifying people with potentially long hair, aka not bald.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c6 c2">The system will load both models and compare the accuracy results generated using the inferencing rules for the long hair which is an upper ontology term which consists of images with the following attributes: Curly Hair, Wavy Hair, Straight Hair, and Bald. The model which produces the higher accuracy will be declared better at classifying people with long hair and the difference between the accuracies will be used to qualify the statement. However, because we do not have a suitably strong positive relationship between any of our attributes and the questions but do have a strong negative relationship &mdash; a matter that we are not going to be responsible for dynamically figuring out because this is far beyond our scope, instead it shall be hardcoded into the pre-arranged question selection &mdash; we can remove results that can&rsquo;t possibly have long hair, bald people for example, and present the results as a potential solution built from the negation of the impossible options.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c2 c12">Q: How well would my model work at classifying mugshot-like photos?</span></p><p class="c0"><span class="c6 c2">A: The model has 90% accuracy on images that match labels that the user associated with mugshots.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c6 c2">Here the user wishes to evaluate the model&rsquo;s use in a particular case. While the LFW images do not contain mugshots, the attributes do contain several attributes which appear in mugshot photos as well. The user enters in a list of attributes (some of which may be higher-level attributes in our taxonomy tree like &ldquo;facial hair.&rdquo; In the mugshot case these attributes would likely be &ldquo;Harsh lighting&rdquo;, &ldquo;Posed photo&rdquo;, and &ldquo;Eyes open.&rdquo; The system would then, using our ontological taxonomy tree, find all of the photos that are tagged with these attributes or fall into a higher level attribute such as &ldquo;facial hair&rdquo;. Then using an inferencing rule we will identify misclassification and calculate the models accuray on this subset of images. After calculating this accuracy another inferencing rule will rate the model great, good, ok, fair, or bad depending on the accuracy. &nbsp;Finally the system will display those results to the user in our UI.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c34 c37"><br></span><span class="c67 c37 c60">Q: What type of occlusions does my facial recognition model misclassify most?</span></p><p class="c0"><span class="c6 c2">A: Your model is most likely to fail when the eyes are covered.</span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c6 c2">We consider the above question to be semantically similar to, &ldquo;What part of the face does my model have trouble handling when it is occluded?&rdquo; In our model, we have defined Occlusion as a class. In addition, we have defined various subclasses of Occlusion that are differentiated by what part of the face they are blocking. When running a model&rsquo;s result set with this question in mind, we will have access to the ground truth of what is truly correct, and access to what the model believes is correct. By comparing these, we will now have access to what classification the model in question got incorrect. Because we have specified that the models being used with our system must have been trained on the LFW training set, we also have access to what accessories, attributes, and &mdash; with a bit of inferencing&mdash; sources of occlusion are in an image. [Note that images are pre-tagged with what accessories a person is hearing. With that, we can do &ldquo;insert-aforementioned-accessory-here&rdquo; isOcclusionSourceOf &ldquo;resultant-Occlusion&rdquo;] Because the occlusions are associated with a specific region of the face, this means that you can now find the associated facial region. [&ldquo;resultant-Occlusion&rdquo; isOccludingBodyRegion &ldquo;facial-region&rdquo;] Throw in a couple sprinkles of math and now you have the answer.</span></p><hr style="page-break-before:always;display:none;"><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c0"><span class="c26">VIII. Resources</span></p><p class="c0 c9"><span class="c15 c84"></span></p><p class="c0"><span class="c26">Knowledge Bases, Repositories, or other Data Sources</span></p><a id="t.711d3edf8bd109aed9b5256b829bf64c149fdfee"></a><a id="t.4"></a><table class="c45"><tbody><tr class="c61"><td class="c28 c68" colspan="1" rowspan="1"><p class="c0"><span class="c48">Data</span></p></td><td class="c28 c102" colspan="1" rowspan="1"><p class="c0"><span class="c48">Type</span></p></td><td class="c28 c66" colspan="1" rowspan="1"><p class="c0"><span class="c48">Characteristics</span></p></td><td class="c28 c73" colspan="1" rowspan="1"><p class="c0"><span class="c48">Description</span></p></td><td class="c28 c97" colspan="1" rowspan="1"><p class="c16"><span class="c48">Owner</span></p></td><td class="c28 c80" colspan="1" rowspan="1"><p class="c16"><span class="c48">Source</span></p></td><td class="c28 c93" colspan="1" rowspan="1"><p class="c16"><span class="c48">Access Policies &amp; Usage</span></p></td></tr><tr class="c31"><td class="c85" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Labeled Faces in the Wild (LFW)</span></p></td><td class="c75" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">local</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">13,000 labeled images of faces collected from the web</span></p><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">A database of labeled face images designed for studying facial recognition, along with the </span><span class="c33">ground truth</span><span class="c6 c2">&nbsp;answers. It&rsquo;s being used because it is a community standard in domain of facial recognition.</span></p><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c50" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">University of Massachusetts Amherst and NSF</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c0"><span class="c33 c41">http://vis-www.cs.umass.edu/lfw/index.html</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">NSF requires projects funded by them to share their data &amp; UMass openly shares it</span></p></td></tr><tr class="c31"><td class="c85" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">LFW Attribute</span></p></td><td class="c75" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">local</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">~77 macro attributes labeled on every image</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Image attributes learned for every image within the LFW dataset. Image ids will overlap with allowing us to associate a attributes to an image.</span></p></td><td class="c50" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Columbia University and NSF</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c0"><span class="c33 c40">http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Provided alongside LFW</span></p></td></tr><tr class="c31"><td class="c85" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">FaceNet</span></p></td><td class="c75" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">local</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Has achieved an accuracy of 99.63% on LFW</span></p></td><td class="c90" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">A convolutional neural network model trained on the images from the LFW dataset. &nbsp;Image ids on test results will overlap with LFW ids allowing us to determine ground truth and attributes</span></p></td><td class="c50" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Google</span></p></td><td class="c79" colspan="1" rowspan="1"><p class="c0"><span class="c33 c40">https://github.com/davidsandberg/facenet/wiki/Validate-on-lfw</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">MIT License (unlimited copy, modify, merge, publish, &hellip;)</span></p></td></tr><tr class="c31"><td class="c86" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">dlib</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">local</span></p></td><td class="c107" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Has achieved an accuracy of 99.38% on LFW</span></p></td><td class="c94" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">A residual neural network model trained on the images from the LFW dataset</span></p></td><td class="c105" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Davis E. King</span></p></td><td class="c89" colspan="1" rowspan="1"><p class="c0"><span class="c33 c40">https://github.com/davisking/dlib</span></p></td><td class="c42" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Boost Software License - Version 1.0 - August 17th, 2003</span></p></td></tr></tbody></table><p class="c46 c9"><span class="c4"></span></p><p class="c0"><span class="c26">External Ontologies, Vocabularies, or other Model Services</span></p><a id="t.8887cfe66693e3e30f8570290f5a1623c705ce27"></a><a id="t.5"></a><table class="c45"><tbody><tr class="c7"><td class="c28 c71" colspan="1" rowspan="1"><p class="c0"><span class="c48">Resource</span></p></td><td class="c28 c59" colspan="1" rowspan="1"><p class="c0"><span class="c48">Language</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c16"><span class="c48">Description</span></p></td><td class="c28 c43" colspan="1" rowspan="1"><p class="c16"><span class="c48">Owner</span></p></td><td class="c28 c44" colspan="1" rowspan="1"><p class="c16"><span class="c48">Source</span></p></td><td class="c28 c103" colspan="1" rowspan="1"><p class="c0"><span class="c48">Describes/Uses</span></p></td><td class="c28 c87" colspan="1" rowspan="1"><p class="c0"><span class="c48">Access Policies &amp; Usage</span></p></td></tr><tr class="c7"><td class="c52" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">VoID</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">RDFS</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">An ontology to express metadata about a dataset</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">DERI/W3C</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">http://vocab.deri.ie/void#Dataset</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">VoID is used in over 77 other ontologies</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Open</span></p></td></tr><tr class="c7"><td class="c52" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">VCO</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">OWL</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">An ontology that organize visual features in a hierarchical structure to described image content.</span></p><p class="c0 c9"><span class="c6 c2"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">DISA</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">http://disa.fi.muni.cz/results/software/visual-concept-ontology/</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">WordNet used to describe concepts within VCO</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Open</span></p></td></tr><tr class="c7"><td class="c52" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">MLS</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">OWL</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">An ontology that can be used to represent and information on data mining and machine learning algorithms, datasets, and experiments.</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">W3C working group</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c0"><span class="c33 c41"><a class="c3" href="https://www.google.com/url?q=http://htmlpreview.github.io/?https://github.com/ML-Schema/documentation/blob/gh-pages/ML%2520Schema.html&amp;sa=D&amp;ust=1544332981755000">http://htmlpreview.github.io/?https://github.com/ML-Schema/documentation/blob/gh-pages/ML%20Schema.html</a></span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">None yet, it&rsquo;s a new ontology working towards becoming a W3C standard</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Open</span></p></td></tr><tr class="c7"><td class="c52" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">LIO</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">OWL</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">An ontology that describes the visual features and attributes of any visual element</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Image Snippet</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c0"><span class="c33 c40">http://www.imagesnippets.com/lio/lio.owl</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c34 c37">Used by image snippet to tag images for smart searching</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c0"><span class="c6 c2">Open</span></p></td></tr></tbody></table><hr style="page-break-before:always;display:none;"><p class="c9 c46"><span class="c4"></span></p><p class="c0"><span class="c26">IX. References and Bibliography</span></p><p class="c0 c9"><span class="c4"></span></p><p class="c0"><span class="c15 c2">[1] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, &ldquo;Labeled faces in the wild: A database for studying face recognition in unconstrained environments,&rdquo; in Workshop on faces in&rsquo;Real-Life&rsquo;Images: detection, alignment,</span></p><p class="c0"><span class="c15 c2">and recognition, 2008.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[2] F. Schroff, D. Kalenichenko, and J. Philbin, &ldquo;Facenet: A unified embedding for face recognition and clustering,&rdquo; in Proceedings of the IEEE conference on computer vision and pattern recognition , 2015, pp. 815&ndash;823.</span></p><p class="c0 c9"><span class="c2 c15"></span></p><p class="c0"><span class="c15 c2">[3] N. Kumar, A. Berg, P. N. Belhumeur, and S. Nayar, &ldquo;Describable visual attributes for face verification and image search,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 1962&ndash;1977, 2011.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, &ldquo;Imagenet: A large-scale hierarchical image database,&rdquo; in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. Ieee, 2009, pp. 248&ndash;255.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[5] M. Johnson and A. Savakis, &ldquo;L1-grassmann manifolds for robust face recognition,&rdquo; in Image Processing (ICIP), 2015 IEEE International Conference on. IEEE, 2015, pp. 482&ndash;486.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c37">[6] Imagesnippets. [Online]. Available: </span><span class="c57 c37"><a class="c3" href="https://www.google.com/url?q=http://www.imagesnippets.com/&amp;sa=D&amp;ust=1544332981760000">www.imagesnippets.com/</a></span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[7] D. E. King, &ldquo;Dlib-ml: A machine learning toolkit,&rdquo; Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1755&ndash;1758, 2009.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[8] E. Learned-Miller, G. B. Huang, A. RoyChowdhury, H. Li, and G. Hua, &ldquo;Labeled faces in the wild: A survey,&rdquo; in Advances in face detection and facial image analysis. Springer, 2016, pp. 189&ndash;248.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">[9] D. Silverstein and J. Farrell, &ldquo;The relationship between image fidelity and image quality,&rdquo; Proceedings of 3rd IEEE International Conference on Image Processing.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0 c9"><span class="c6 c2"></span></p><p class="c46 c9"><span class="c4"></span></p><p class="c0"><span class="c26">X. Notes</span></p><p class="c0 c9"><span class="c4"></span></p><p class="c0"><span class="c15 c2">Model accuracy values are more specifically model accuracy on the LFW dataset. Obviously we cannot test the model on all possible images.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">Both LFW and LFW Attributes contain ground truths for the ontology to use for computing accuracy.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">Misclassifications associated with Occlusions</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 333.33px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 333.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c15 c2">This is an example hierarchy view of the various forms of occlusion that occured in a test set. In this view, the user is given the various forms of expected occlusion organized into an &ldquo;is-a&rdquo; based tree. In the above diagram, the numbers in the circles are the percentage of misclassifications that have occured on an image that has been tagged with that attribute compared to the total tagged for that attribute. For example, exactly 47% of the images that are tagged as contained lipstick were correctly classified. Thus a person could assume that this was a major issue and it should be addressed, aka, receive a score of, &ldquo;bad.&rdquo; Only 82% of bald faces were correctly classified when the average for all forms of hair is 99, thus the model has an issue with bald people. We are currently considering adding inference rules for stings like statistical significance, bias, etc based on the essentials of research statistics.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c2 c60 c95">Example Inferencing rules in Blazegraph</span></p><p class="c0"><span class="c37 c57"><a class="c3" href="https://www.google.com/url?q=https://wiki.blazegraph.com/wiki/index.php/InferenceAndTruthMaintenance&amp;sa=D&amp;ust=1544332981762000">https://wiki.blazegraph.com/wiki/index.php/InferenceAndTruthMaintenance</a></span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c37">RDFS10 is: (?u,rdfs:subClassOf,?u) &lt;= (?u,rdf:type,rdfs:Class).<br><br></span><span class="c37 c96">public class RuleRdfs10 extends Rule {<br> &nbsp; &nbsp;private static final long serialVersionUID = -2964784545354974663L;<br> &nbsp; &nbsp;public RuleRdfs10(String relationName, Vocabulary vocab) {<br> &nbsp; &nbsp; &nbsp; &nbsp;super( &nbsp;&quot;rdfs10&quot;,<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate(relationName,var(&quot;u&quot;), vocab.getConstant(RDFS.SUBCLASSOF), var(&quot;u&quot;)),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate[]{<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate(relationName,var(&quot;u&quot;), vocab.getConstant(RDF.TYPE), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vocab.getConstant(RDFS.CLASS))},<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;null // constraints<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;);<br> &nbsp; &nbsp;} &nbsp; &nbsp;<br>}<br><br></span><span class="c15 c2">Once you have written your own rule, you need to incorporate it into one of the &quot;inference programs&quot;, typically the FullClosure program.</span></p><p class="c0 c9"><span class="c15 c2"></span></p><p class="c0"><span class="c15 c2">For example we could easily use this API to add additional rules such as:</span></p><p class="c0 c70"><span class="c37 c60">Correct Classification</span><span class="c15 c2">&nbsp;by querying for Result classes that have a prov:value that matches to who the Image is prov:attributedTo</span></p><p class="c0 c70 c9"><span class="c15 c2"></span></p><p class="c0 c70"><span class="c37 c60">Accuracy Ratings</span><span class="c15 c2">&nbsp;by querying for accuracy and asserting a label {good, bad, okay} if were within some range </span></p><p class="c0 c9 c70"><span class="c15 c2"></span></p><p class="c0 c70"><span class="c37">We haven&#39;t had a chance to implement any of these additional rules yet, but plan to in the near future.</span></p></body></html>