<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=7ebMze0MJP7bZu91CQ7uEApNHmxKgM3Y09dQPiEfh2Y');ol.lst-kix_list_1-3{list-style-type:none}ol.lst-kix_list_1-4{list-style-type:none}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_list_1-1{list-style-type:none}ol.lst-kix_list_1-2{list-style-type:none}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ". "}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,lower-latin) ". "}.lst-kix_list_1-2>li:before{content:"" counter(lst-ctn-kix_list_1-2,lower-roman) ". "}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_1-8{list-style-type:none}.lst-kix_list_1-3>li:before{content:"" counter(lst-ctn-kix_list_1-3,decimal) ". "}.lst-kix_list_1-4>li:before{content:"" counter(lst-ctn-kix_list_1-4,lower-latin) ". "}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_list_1-7>li:before{content:"" counter(lst-ctn-kix_list_1-7,lower-latin) ". "}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:"" counter(lst-ctn-kix_list_1-5,lower-roman) ". "}.lst-kix_list_1-6>li:before{content:"" counter(lst-ctn-kix_list_1-6,decimal) ". "}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}.lst-kix_list_1-8>li:before{content:"" counter(lst-ctn-kix_list_1-8,lower-roman) ". "}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}ol{margin:0;padding:0}table td,table th{padding:0}.c13{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#000000;border-bottom-style:solid}.c1{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:467.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c45{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:274.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c43{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:391.1pt;border-top-color:#4f81bd;border-bottom-style:solid}.c47{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:467.1pt;border-top-color:#4f81bd;border-bottom-style:solid}.c25{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#4f81bd;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#000000;border-bottom-style:solid}.c35{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#4f81bd;border-bottom-style:solid}.c72{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#000000;border-bottom-style:solid}.c93{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:121.5pt;border-top-color:#000000;border-bottom-style:solid}.c84{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#4f81bd;border-bottom-style:solid}.c100{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:45pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c97{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:240.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c11{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:389.4pt;border-top-color:#4f81bd;border-bottom-style:solid}.c77{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#000000;border-bottom-style:solid}.c61{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c91{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#000000;border-bottom-style:solid}.c74{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:54.2pt;border-top-color:#000000;border-bottom-style:solid}.c104{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c68{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c2{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#4f81bd;border-bottom-style:solid}.c32{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c22{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:274.6pt;border-top-color:#4f81bd;border-bottom-style:solid}.c85{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:53.5pt;border-top-color:#000000;border-bottom-style:solid}.c36{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:95.2pt;border-top-color:#000000;border-bottom-style:solid}.c89{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.8pt;border-top-color:#000000;border-bottom-style:solid}.c62{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#000000;border-bottom-style:solid}.c39{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:116.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c53{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:141pt;border-top-color:#4f81bd;border-bottom-style:solid}.c20{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c59{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:240.3pt;border-top-color:#000000;border-bottom-style:solid}.c69{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c28{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c80{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:463.5pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:97.5pt;border-top-color:#000000;border-bottom-style:solid}.c71{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#4f81bd;border-bottom-style:solid}.c98{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:79.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c96{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:389.4pt;border-top-color:#4f81bd;border-bottom-style:solid}.c14{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#000000;border-bottom-style:solid}.c57{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c88{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:97.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c75{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:32.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c94{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.5pt;border-top-color:#000000;border-bottom-style:solid}.c102{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:72pt;border-top-color:#000000;border-bottom-style:solid}.c29{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#4f81bd;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.8pt;border-top-color:#4f81bd;border-bottom-style:solid}.c60{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:121.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c66{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#000000;border-bottom-style:solid}.c30{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:54.2pt;border-top-color:#4f81bd;border-bottom-style:solid}.c9{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#4f81bd;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:77.7pt;border-top-color:#4f81bd;border-bottom-style:solid}.c76{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:53.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c63{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#4f81bd;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:78.3pt;border-top-color:#4f81bd;border-bottom-style:solid}.c18{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:31.5pt;border-top-color:#000000;border-bottom-style:solid}.c78{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:48.5pt;border-top-color:#4f81bd;border-bottom-style:solid}.c33{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:238.5pt;border-top-color:#000000;border-bottom-style:solid}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c15{-webkit-text-decoration-skip:none;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Times New Roman";font-style:normal}.c65{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c55{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c95{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:right}.c87{padding-top:0pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c8{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c27{font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Times";font-style:normal}.c48{text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c12{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times";font-style:normal}.c24{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:italic}.c92{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial"}.c58{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c16{margin-left:-5.8pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c90{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c34{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c4{font-size:9pt;color:#ffffff;font-weight:700}.c52{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c21{font-size:10pt;font-family:"Ubuntu";font-weight:400}.c26{color:#ff0000;font-size:10pt}.c49{margin-left:40.5pt;text-indent:-40.5pt}.c5{color:#000000;font-size:10pt}.c81{color:#000000;font-size:12pt}.c17{font-family:"Times";font-weight:400}.c86{vertical-align:baseline;font-style:normal}.c51{margin-left:15.2pt;padding-left:0pt}.c38{color:#ffffff;font-weight:700}.c83{color:#ff0000;font-size:12pt}.c79{color:#000000;font-size:8pt}.c50{color:inherit;text-decoration:inherit}.c40{color:#000000;font-weight:700}.c99{margin-left:72pt;text-indent:-72pt}.c67{padding:0;margin:0}.c70{height:11pt}.c103{height:15pt}.c101{height:27pt}.c46{font-style:italic}.c6{height:12pt}.c56{font-family:"Times"}.c54{background-color:#4f81bd}.c73{color:#1155cc}.c44{height:13pt}.c64{color:#0563c1}.c37{height:3pt}.c31{font-size:10pt}.c41{height:51pt}.c10{height:0pt}.c82{height:14pt}.c42{font-weight:700}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c34"><div><p class="c95"><span class="c8 c79">Team FRMA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew Johnson // Alexander Schwartzberg // John Angel // Jordan Faas-Bush // Andrew Gaudet</span></p></div><p class="c6 c90" id="h.gjdgxs"><span class="c86 c92"></span></p><a id="t.a2a4967e6fd70144df120c4bffab96be507e333d"></a><a id="t.0"></a><table class="c16"><tbody><tr class="c44"><td class="c43" colspan="2" rowspan="1"><p class="c3"><span class="c38">I. Use Case Description</span></p></td></tr><tr class="c70"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Use Case Name</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c5 c46">Facial Recognition Model Analyzer</span></p></td></tr><tr class="c82"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Use Case Identifier</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c0">1</span></p></td></tr><tr class="c44"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Source</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3 c6"><span class="c0"></span></p></td></tr><tr class="c70"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Point of Contact</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c5 c46">Matthew Johnson, </span><span class="c52 c46 c73 c31"><a class="c50" href="mailto:johnsm21@rpi.edu">johnsm21@rpi.edu</a></span></p><p class="c3"><span class="c46 c31">Alexander Schwartzberg, </span><span class="c52 c46 c73 c31"><a class="c50" href="mailto:schwaa6@rpi.edu">schwaa6@rpi.edu</a></span></p><p class="c3"><span class="c46 c31">John Angel, </span><span class="c52 c46 c73 c31"><a class="c50" href="mailto:angelj3@rpi.edu">angelj3@rpi.edu</a></span><span class="c24 c5">&nbsp;</span></p><p class="c3"><span class="c46 c31">Jordan Faas-Bush, </span><span class="c52 c46 c73 c31"><a class="c50" href="mailto:faasbj@rpi.edu">faasbj@rpi.edu</a></span></p><p class="c3"><span class="c46 c31">Andrew Gaudet, </span><span class="c52 c46 c73 c31"><a class="c50" href="mailto:gaudea@rpi.edu">gaudea@rpi.edu</a></span></p></td></tr><tr class="c70"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Creation / Revision Date</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c5">September 22, 2018</span></p></td></tr><tr class="c10"><td class="c39" colspan="1" rowspan="1"><p class="c3"><span class="c5">Associated Documents</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c8 c5">https://tw.rpi.edu/web/Courses/Ontologies/2018/FRMA</span></p></td></tr></tbody></table><p class="c3 c6"><span class="c0"></span></p><a id="t.189df6ac3407bda99031ee93256084226e7700de"></a><a id="t.1"></a><table class="c16"><tbody><tr class="c10"><td class="c47" colspan="2" rowspan="1"><p class="c3"><span class="c38">II. Use Case Summary</span></p></td></tr><tr class="c103"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Goal</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Machine learning allows us to learn a model for a given task such as facial recognition with a high degree of accuracy. However, after these models are generated they are often treated as black boxes and the limitations of a model are often unknown to the end user. The system developed for this use case will provide an intuitive interface to explore the limits of a facial recognition model by semantically integrating &ldquo;smart&rdquo; images(semantically describe who the image depicts and what Kumar [2] features the image exhibits) with classification results to discover common causes for misclassifications.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Requirements</span></p></td><td class="c11" colspan="1" rowspan="1"><ol class="c67 lst-kix_list_1-0 start" start="1"><li class="c3 c51"><span class="c5 c17">Need an ontology describing the hierarchy of image features from Kumar et al. paper [3] (the ontology will be designed and implemented as part of this project)</span><span class="c17 c31">. The ontology will be designed and implemented as part of this project.</span></li><li class="c3 c51"><span class="c5 c17">Needs to able load test results for a given model and link the results to images from the </span><span class="c17 c31">Labeled Faces in the Wild (</span><span class="c5 c17">LF</span><span class="c17 c31">W)</span><span class="c12 c5">&nbsp;dataset [1]</span></li></ol></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Scope</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">The scope for this use case is limited to the LFW dataset, the LFW ground truth [1] and a single facial recognition model FaceNet [2]. In addition, we will only consider image attributes learned in Kumar et al. paper [3]</span><span class="c5 c17">.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Priority</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c5 c46">Identify the priority of the use case (with respect to other use cases for the project)</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Stakeholders</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31 c52">Machine Learning Researchers</span><span class="c12 c5">- Did your new methodology develop a model that covers the training domain specifically or can it be easily transferred to new domains?</span></p><p class="c3"><span class="c52 c17 c31">Application Engineer or Program Manager</span><span class="c17 c31">- You have ten different machine learning models you could include in your product and no test data; how do you choose which to include?</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Description</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c5 c17">The use case is focused on requirements for the development of an application to support fac</span><span class="c17 c31">ial</span><span class="c5 c17">&nbsp;recognition model analysis. It will be used to discover how an ontology of image attributes correlate to the test results of a fac</span><span class="c17 c31">ial</span><span class="c12 c5">&nbsp;recognition model. The application must be able to read in model test results provided by a user and link classification results to known &ldquo;smart&rdquo; images which have a hierarchical ontology of visual features describing the image content. Using this alignment between datasets the system will be able to calculate accuracy statistics across images attributes and find correlations between misclassifications and image attributes.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c5 c17">The system will present this information as a semantically enhanced hierarchy view of attributes with the statistics of all images that contain that attribute (i</span><span class="c17 c31">.e &lsquo;</span><span class="c5 c17">has goatee</span><span class="c17 c31">&rsquo;</span><span class="c5 c17">) or belong under an attribute category (i.e. </span><span class="c17 c31">&lsquo;</span><span class="c5 c17">has facial hair</span><span class="c17 c31">&rsquo;</span><span class="c5 c17">). When a user selects a level on this category they will be presented with a view showing the images associated with that category and an indication of whether they were classified correctly or not. In addition, the user should be able to switch between all images, correctly classified images, and misclassifications which cause the hierarchy view to update statistics and the images presented.</span><span class="c5">&nbsp;</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Actors / Interfaces</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer (person who wants to know more about a model)</span></p><p class="c3"><span class="c12 c5">Labeled Faces in the Wild Dataset [1]</span></p><p class="c3"><span class="c12 c5">The test results of the model being evaluated.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Pre-conditions</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The system must have the image attributes lifted into RDF and aligned with the feature ontology.</span></p><p class="c3"><span class="c12 c5">The model results that the user is inputting into the system must have used the LFW dataset as their training dataset.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Post-conditions</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Postconditions (Success) - The user loads their model&rsquo;s test results into the Model Analyzer to generate a hierarchical view of misclassifications and notices that model misclassified facial hair the most and examines the various misclassified photos.</span></p><p class="c3 c6 c99"><span class="c12 c5"></span></p><p class="c3"><span class="c17 c31">Postconditions</span><span class="c56 c31 c42">&nbsp;</span><span class="c12 c5">(Failure) - The user tries to load their model&rsquo;s test results into the Model Analyzer, but the file format was incorrect. As a result, the user was unable to generate a hierarchical view.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Triggers</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">A user submits the test results for a model and selects the model from which the results were generated, or a user selects a previously uploaded test result set.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Performance Requirements</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c5 c46">List any known performance-specific requirements &ndash; timing and sizing (volume, frequency, etc.), maintainability, reusability, other &ldquo;-ilities&rdquo;, etc.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Assumptions </span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3"><span class="c31">Image attributes will correlate with the performance of the model being examined. Past experiments have shown that image factors such as quality and facial occlusion reduce the accuracy of facial recognition [5], but there&rsquo;s no guarantee the facenet classification results will reflect that.</span></p></td></tr><tr class="c10"><td class="c9" colspan="1" rowspan="1"><p class="c3"><span class="c5">Open Issues</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c3 c6"><span class="c0"></span></p></td></tr></tbody></table><p class="c55 c6"><span class="c0"></span></p><p class="c3"><span class="c40">III. Usage Scenarios</span></p><p class="c3 c6"><span class="c24 c26"></span></p><p class="c3"><span class="c5 c17">A user wants to learn the limitations of their new fa</span><span class="c17 c31">cial</span><span class="c5 c17">&nbsp;recognition model, so they run the model against a test set and capture the classification result of each picture in a </span><span class="c17 c31">CSV </span><span class="c5 c17">file. The user th</span><span class="c17 c31">e</span><span class="c5 c17">n takes this file and imports it into the Model Analyzer web portal and chooses the dataset the model was run against. The system then lifts the results into RDF and links each test result to a &ldquo;smart&rdquo; image which has the image attributes described in an ontology. The system then generates a </span><span class="c12 c5">semantically enhanced hierarchy view of attributes with the classification accuracy of images that contain that attribute. The user then switches the system from &lsquo;all&rsquo; to &lsquo;misclassification only&rsquo; and the hierarchy view updates to show attribute misclassification accuracy. From this view the user notices that the model misclassifies images with facial hair the most and clicks on this level of the hierarchy. The portal than populates the screen with the test images from that attribute level that were misclassified, allowing further investigation.</span></p><p class="c3 c6"><span class="c24 c26"></span></p><p class="c3"><span class="c5 c17">A user wants to know if </span><span class="c17 c31">their </span><span class="c5 c17">previously analyzed </span><span class="c17 c31">facial recognition</span><span class="c5 c17">&nbsp;model would do well on images with poor lighting. The user opens the Model Analyzer web portal and chooses a previously run data model from the list. The system then loads in the previous results into RDF and links each test result to a &ldquo;smart&rdquo; image which has the image attributes described in an ontology. The system then generates a </span><span class="c12 c5">semantically enhanced hierarchy view of attributes with counts of the number of images that contain that attribute. From this view the user selects both the harsh lighting and soft lighting image attributes. The portal then populates the screen with the test images that contain either of those attributes and a combined accuracy. The user leverages this accuracy figure to conclude that their old model will work well in poor lighting conditions.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c87"><span class="c48 c40">IV. Basic Flow of Events</span></p><p class="c87 c6"><span class="c0"></span></p><a id="t.d26ecc0077df9c0c654710edfc8a43fff7731452"></a><a id="t.2"></a><table class="c16"><tbody><tr class="c10"><td class="c1" colspan="4" rowspan="1"><p class="c3"><span class="c38">Basic / Normal Flow of Events</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Step</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Actor (Person)</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Actor (System)</span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Description</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">1</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User accesses Model Analyzer portal</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">2</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">User selects model used to generate test results </span><span class="c27 c5">(FaceNet, dlib)</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">2</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User uploads new model test results</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">3</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User selects corresponding test dataset from list.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">4</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Portal retrieves data from attribute ontology and integrates model test results</span></p></td></tr><tr class="c37"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">5</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user&rsquo;s browser renders a semantically enhanced hierarchy view of attributes with counts of number of images with that attribute. The system will use the ground truth data in the LFW Attributes dataset to associate images used in the user&rsquo;s test results to Kumar[2] attributes. Then it would simply count the occurrences of attributes in the users test results. Additional note: the &ldquo;hierarchy view&rdquo; structure is directly taken from the attribute ontology that we will be building.</span></p></td></tr><tr class="c37"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">6</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User selects misclassifications from a dropdown menu</span></p></td></tr><tr class="c37"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">7</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user&rsquo;s browser updates the hierarchy so the counts show the misclassification on each attribute level. The system accomplishes this by first accessing the ground truths within the LFW dataset and comparing them to the inputted test results to obtain which images were correctly identified in the test results. This information refined by an inference rule that compares the TestResult with the ground truth and assert if the model classified the image correctly.. The system will then take this information and combine it with the LFW Attributes dataset&rsquo;s ground truths to associate those test results to image attributes. From here only results that were misclassified will be shown.</span></p></td></tr><tr class="c37"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">8</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user selects some level on the hierarchy.</span></p></td></tr><tr class="c37"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">9</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c59" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The portal shows all images with that attribute level that were misclassified. This will be accomplished through similar logic to step 7 with the exception that only results of a certain level&rsquo;s class will be show.</span></p></td></tr></tbody></table><p class="c3 c6"><span class="c0"></span></p><p class="c87"><span class="c40">V. Alternate Flow of Events</span></p><p class="c3 c6"><span class="c0"></span></p><a id="t.217f838a927f861d18289d93655b9ff0bb4db90c"></a><a id="t.3"></a><table class="c16"><tbody><tr class="c10"><td class="c54 c80" colspan="4" rowspan="1"><p class="c3"><span class="c38">Alternate Flow of Events</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Step</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Actor (Person)</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Actor (System)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c5 c42">Description</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">1</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User accesses Model Analyzer portal.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">2</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">User selects model used to generate test results </span><span class="c5 c27">(FaceNet, dlib)</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">2</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c27 c5">User selects previously uploaded model test results.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c5 c12">3</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User selects corresponding test dataset from list.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">4</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Portal retrieves data from attribute ontology and integrates model test results.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">5</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user&rsquo;s browser renders semantically enhanced hierarchy view of attributes, which displays a tree view showing top level image characteristic such as lighting variation, depicted person attributes, along with mid-level attributes such as wearables, and low level attributes like hats. An example tree is shown in the notes section below. Across each level of this tree we will collect counts of number of images with that attribute. The system will use the ground truth data in the LFW Attributes dataset to associate images used in the user&rsquo;s test results to Kumar attributes. Then it would simply count the occurrences of attributes in the users test results. Additional note: the &ldquo;hierarchy view&rdquo; structure is directly taken from the attribute ontology that we will be building.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">6</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">User selects misclassifications from a dropdown menu</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">7</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user&rsquo;s browser updates the hierarchy so the counts show the misclassification on each attribute level. The system accomplishes this by first accessing the ground truths within the LFD dataset and comparing them to the inputted test results to obtain which images were correctly identified in the test results. This information refined by some inference rules will give which images were misclassified. For example if we get a test result that says image 203 is Steve Irwin, we can perform a query to get the ground truth data on image 203 and if that image depicts Steve Irwin, we will assert a new triple saying that the classification result is correct.The system will then take this information and combine it with the LFW Attributes dataset&rsquo;s ground truths to associate those test results to image attributes. From here only results that were misclassified will be shown.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">8</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Researcher/Engineer</span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The user selects some level on the hierarchy.</span></p></td></tr><tr class="c10"><td class="c18" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">9</span></p></td><td class="c14" colspan="1" rowspan="1"><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c36" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Model Analyzer App</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">The portal shows all images with that attribute level that were misclassified. This will be accomplished through similar logic to &nbsp;Step 7 with the exception that only results of a certain level&rsquo;s class will be shown. An example of what the hierarchy view will look like is shown in the notes section.</span></p><p class="c3 c6"><span class="c12 c5"></span></p></td></tr></tbody></table><hr style="page-break-before:always;display:none;"><p class="c55 c6"><span class="c0"></span></p><p class="c3"><span class="c40 c48">VI. Use Case and Activity Diagram(s)</span></p><p class="c65 c6"><span class="c0"></span></p><p class="c65"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.57px; height: 612.50px;"><img alt="" src="images/image1.png" style="width: 439.57px; height: 612.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c65"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 358.67px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 358.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c65"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 551.00px; height: 446.48px;"><img alt="" src="images/image2.png" style="width: 551.00px; height: 446.48px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c40">VII. Competency Questions</span></p><p class="c3 c6"><span class="c0"></span></p><p class="c3"><span class="c27 c5">Q: What type of image attributes does the model have the most trouble classifying?</span></p><p class="c3"><span class="c12 c5">A: 45% of misclassification occurs on images that contain facial hair.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c17 c31">Prior to the triggers the system will consist of several components: LFW[1] smart images which semantically describe who the image depicts and what Kumar [2] features the image in a similar to Image Snippet[6] and a visual concept ontology that describes a taxonomy of Kumar [2] features. Model test results will be loaded into the system and will be lifted to RDF. The system will then use an inference rule and the smart images to determine if an images within the test result are classified correctly or misclassified. For example if we get a test result that says image 203 is Steve Irwin, we can perform a query to get the ground truth data on image 203 and if that image depicts Steve Irwin, we will assert a new triple saying that the classification result is correct.Another inference rule will calculate the accuracy of classified images with an attribute by counting the number of correctly classified images with the attribute and dividing by all images with the attribute. This process will be repeated for each attribute and upper level term within the ontology of visual concepts to calculate accuracy. Afterwards another inferencing rule will examine the accuracies and provide a rating of great, good, ok, fair, bad. Finally we will display these results to the end user through the hierarchical view that shows the hierarchy of attributes/upper terms and the accuracy, ratings. The hierarchy view will start by displaying the results for the highest levels of the taxonomy tree and show every image and the accuracy overall, but the user will be able to select a lower level of the tree such as whether the photo used a flash, whether the subject is smiling or not, or, as listed in this competency question, whether or not the subject has facial hair, and the view will only show the results for photos that belong to that section of the hierarchy tree.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c27 c5">Q: What type of image attributes does the model most associate with Johnny Depp?</span></p><p class="c3"><span class="c17 c31">A: &lsquo;Wearable&rsquo; </span><span class="c5 c17">is most associated with </span><span class="c12 c5">Johnny Depp.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c12 c5">Since the model test results are known, we can link them to our &ldquo;smart&rdquo; images that contain attribute tagging. From this we can look at all the images of Johnny Depp that were classified properly using the LFW ground truth and count which attributes occur more frequently by running a SPARQL query counting the number of occurrences of each attribute in images that were classified as &ldquo;Johnny Depp&rdquo; by the user&rsquo;s model - for instance, &ldquo;wearing hat&rdquo; and &ldquo;wearing earrings.&rdquo; Then using our ontology&rsquo;s taxonomy structure for attributes, we can determine that all these attributes are a type of &ldquo;wearable.&rdquo; We then display the results of these queries using the hierarchy view as mentioned before, only this time instead of showing the accuracy statistics and percentages we show the accumulated number of images that the model has classified as Johnny Depp and contains one or more of the associated attributes (or sub attributes). The user is then able to look at the hierarchy to determine which attributes are most associated with Johnny Depp at the level of detail that they are interested in. This query could work for any attribute we (using the Kumar features) or the model have tagged, be it the person&rsquo;s name or an image attribute.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c27 c5">Q: Which of these two models is better at classifying people with long hair?</span></p><p class="c3"><span class="c12 c5">A: FaceNet is 10% better than dlib at classifying people with potentially long hair, aka not bald.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c12 c5">The system will load both models and compare the accuracy results generated using the inferencing rules for the long hair which is an upper ontology term which consists of images with the following attributes: Curly Hair, Wavy Hair, Straight Hair, and Bald. The model which produces the higher accuracy will be declared better at classifying people with long hair and the difference between the accuracies will be used to qualify the statement. However, because we do not have a suitably strong positive relationship between any of our attributes and the questions but do have a strong negative relationship &mdash; a matter that we are not going to be responsible for dynamically figuring out because this is far beyond our scope, instead it shall be hardcoded into the pre-arranged question selection &mdash; we can remove results that can&rsquo;t possibly have long hair, bald people for example, and present the results as a potential solution built from the negation of the impossible options.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c27 c5">Q: How well would my model work at classifying mugshot photos?</span></p><p class="c3"><span class="c12 c5">A: The model has 90% accuracy on images that match labels that the user associated with mugshots.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c12 c5">Here the user wishes to evaluate the model&rsquo;s use in a particular case. While the LFW images do not contain mugshots, the attributes do contain several attributes which appear in mugshot photos as well. The user enters in a list of attributes (some of which may be higher-level attributes in our taxonomy tree like &ldquo;facial hair.&rdquo; In the mugshot case these attributes would likely be &ldquo;Harsh lighting&rdquo;, &ldquo;Posed photo&rdquo;, and &ldquo;Eyes open.&rdquo; The system would then, using our ontological taxonomy tree, find all of the photos that are tagged with these attributes or fall into a higher level attribute such as &ldquo;facial hair&rdquo;. Then using an inferencing rule we will identify misclassification and calculate the models accuray on this subset of images. After calculating this accuracy another inferencing rule will rate the model great, good, ok, fair, or bad depending on the accuracy. &nbsp;Finally the system will display those results to the user in our UI.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c27 c5">Q: What are the &ldquo;dimly lit&rdquo; images that my model has misclassified?</span></p><p class="c3"><span class="c12 c5">A: The UI shows the user the images tagged with dimly lit that the model misclassifies</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c12 c5">It can be important to gather and view images that the model has misclassified (or correctly classified) for presentations, human evaluations, or the model use case evaluation (i.e. if the model is being evaluated for use in a smart door knocker that recognizes faces, the dimly lit photos that are misclassified have to be acceptable failures, something that can only be evaluated by the humans producing the end device). The user uploads their model results and the results are evaluated using the ontology in the same way as the first competency question to get the percentage accuracy results for each of the items in the attribute taxonomy tree. The end user then selects the appropriate attribute in the UI (i.e. &ldquo;dimly lit&rdquo;), and the UI will then display the images categorized by our ontology as belonging to that attribute along with indicators as to whether the user&rsquo;s model classified the images correctly or incorrectly. The user can then choose to only show the misclassified images using a button in the UI, and the interface will stop displaying the images that were classified correctly and then only show the images containing the attribute that were misclassified.</span></p><p class="c3"><span class="c17 c31"><br></span><span class="c31 c42 c56">Q: What part of the face does my facial recognition model depend on the most?</span></p><p class="c3"><span class="c12 c5">A: Your model is most likely to fail when the eyes are covered.</span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c12 c5">We consider the above question to be semantically similar to, &ldquo;What part of the face does my model have trouble handling when it is occluded?&rdquo; In our model, we have defined Occlusion as a class. In addition, we have defined various subclasses of Occlusion that are differentiated by what part of the face they are blocking. When running a model&rsquo;s result set with this question in mind, we will have access to the ground truth of what is truly correct, and access to what the model believes is correct. By comparing these, we will now have access to what classification the model in question got incorrect. Because we have specified that the models being used with our system must have been trained on the LFW training set, we also have access to what accessories, attributes, and &mdash; with a bit of inferencing&mdash; sources of occlusion are in an image. [Note that images are pre-tagged with what accessories a person is hearing. With that, we can do &ldquo;insert-aforementioned-accessory-here&rdquo; isOcclusionSourceOf &ldquo;resultant-Occlusion&rdquo;] Because the occlusions are associated with a specific region of the face, this means that you can now find the associated facial region. [&ldquo;resultant-Occlusion&rdquo; isOccludingBodyRegion &ldquo;facial-region&rdquo;] Throw in a couple sprinkles of math and now you have the answer.</span></p><hr style="page-break-before:always;display:none;"><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c3"><span class="c40">VIII. Resources</span></p><p class="c3 c6"><span class="c8 c83"></span></p><p class="c3"><span class="c40">Knowledge Bases, Repositories, or other Data Sources</span></p><a id="t.711d3edf8bd109aed9b5256b829bf64c149fdfee"></a><a id="t.4"></a><table class="c16"><tbody><tr class="c101"><td class="c25" colspan="1" rowspan="1"><p class="c3"><span class="c4">Data</span></p></td><td class="c54 c62" colspan="1" rowspan="1"><p class="c3"><span class="c4">Type</span></p></td><td class="c54 c72" colspan="1" rowspan="1"><p class="c3"><span class="c4">Characteristics</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c3"><span class="c4">Description</span></p></td><td class="c54 c102" colspan="1" rowspan="1"><p class="c23"><span class="c4">Owner</span></p></td><td class="c54 c66" colspan="1" rowspan="1"><p class="c23"><span class="c4">Source</span></p></td><td class="c54 c91" colspan="1" rowspan="1"><p class="c23"><span class="c4">Access Policies &amp; Usage</span></p></td></tr><tr class="c41"><td class="c28" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Labeled Faces in the Wild (LFW)</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">local</span></p></td><td class="c68" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">13,000 labeled images of faces collected from the web</span></p><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">A database of labeled face images designed for studying facial recognition, along with the </span><span class="c52 c17 c31">ground truth</span><span class="c12 c5">&nbsp;answers. It&rsquo;s being used because it is a community standard in domain of facial recognition.</span></p><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">University of Massachusetts Amherst and NSF</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c31 c64">http://vis-www.cs.umass.edu/lfw/index.html</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">NSF requires projects funded by them to share their data &amp; UMass openly shares it</span></p></td></tr><tr class="c41"><td class="c28" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">LFW Attribute</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">local</span></p></td><td class="c68" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">~77 macro attributes labeled on every image</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Image attributes learned for every image within the LFW dataset. Image ids will overlap with allowing us to associate a attributes to an image.</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Columbia University and NSF</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c86 c64 c31">http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Provided alongside LFW</span></p></td></tr><tr class="c41"><td class="c28" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">FaceNet</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">local</span></p></td><td class="c68" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Has achieved an accuracy of 99.63% on LFW</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">A convolutional neural network model trained on the images from the LFW dataset. &nbsp;Image ids on test results will overlap with LFW ids allowing us to determine ground truth and attributes</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Google</span></p></td><td class="c61" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c86 c64 c31">https://github.com/davidsandberg/facenet/wiki/Validate-on-lfw</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">MIT License (unlimited copy, modify, merge, publish, &hellip;)</span></p></td></tr><tr class="c41"><td class="c78" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">dlib</span></p></td><td class="c75" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">local</span></p></td><td class="c98" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Has achieved an accuracy of 99.38% on LFW</span></p></td><td class="c53" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">A residual neural network model trained on the images from the LFW dataset</span></p></td><td class="c71" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Davis E. King</span></p></td><td class="c104" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c64 c31 c86">https://github.com/davisking/dlib</span></p></td><td class="c69" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Boost Software License - Version 1.0 - August 17th, 2003</span></p></td></tr></tbody></table><p class="c6 c55"><span class="c0"></span></p><p class="c3"><span class="c40">External Ontologies, Vocabularies, or other Model Services</span></p><a id="t.8887cfe66693e3e30f8570290f5a1623c705ce27"></a><a id="t.5"></a><table class="c16"><tbody><tr class="c10"><td class="c54 c74" colspan="1" rowspan="1"><p class="c3"><span class="c4">Resource</span></p></td><td class="c54 c85" colspan="1" rowspan="1"><p class="c3"><span class="c4">Language</span></p></td><td class="c54 c93" colspan="1" rowspan="1"><p class="c23"><span class="c4">Description</span></p></td><td class="c54 c100" colspan="1" rowspan="1"><p class="c23"><span class="c4">Owner</span></p></td><td class="c54 c94" colspan="1" rowspan="1"><p class="c23"><span class="c4">Source</span></p></td><td class="c54 c77" colspan="1" rowspan="1"><p class="c3"><span class="c4">Describes/Uses</span></p></td><td class="c54 c89" colspan="1" rowspan="1"><p class="c3"><span class="c4">Access Policies &amp; Usage</span></p></td></tr><tr class="c10"><td class="c30" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">VoID</span></p></td><td class="c76" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">RDFS</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">An ontology to express metadata about a dataset</span></p></td><td class="c84" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">DERI/W3C</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">http://vocab.deri.ie/void#Dataset</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">VoID is used in over 77 other ontologies</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Open</span></p></td></tr><tr class="c10"><td class="c30" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">VCO</span></p></td><td class="c76" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">OWL</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">An ontology that organize visual features in a hierarchical structure to described image content.</span></p><p class="c3 c6"><span class="c12 c5"></span></p></td><td class="c84" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">DISA</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">http://disa.fi.muni.cz/results/software/visual-concept-ontology/</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">WordNet used to describe concepts within VCO</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Open</span></p></td></tr><tr class="c10"><td class="c30" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">MLS</span></p></td><td class="c76" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">OWL</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">An ontology that can be used to represent and information on data mining and machine learning algorithms, datasets, and experiments.</span></p></td><td class="c84" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">W3C working group</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c64 c31"><a class="c50" href="https://www.google.com/url?q=http://htmlpreview.github.io/?https://github.com/ML-Schema/documentation/blob/gh-pages/ML%2520Schema.html&amp;sa=D&amp;ust=1544236211341000">http://htmlpreview.github.io/?https://github.com/ML-Schema/documentation/blob/gh-pages/ML%20Schema.html</a></span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">None yet, it&rsquo;s a new ontology working towards becoming a W3C standard</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Open</span></p></td></tr><tr class="c10"><td class="c30" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">LIO</span></p></td><td class="c76" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">OWL</span></p></td><td class="c60" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">An ontology that describes the visual features and attributes of any visual element</span></p></td><td class="c84" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Image Snippet</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c3"><span class="c52 c17 c86 c64 c31">http://www.imagesnippets.com/lio/lio.owl</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c3"><span class="c17 c31">Used by image snippet to tag images for smart searching</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c3"><span class="c12 c5">Open</span></p></td></tr></tbody></table><hr style="page-break-before:always;display:none;"><p class="c55 c6"><span class="c0"></span></p><p class="c3"><span class="c40">IX. References and Bibliography</span></p><p class="c3 c6"><span class="c0"></span></p><p class="c3"><span class="c8 c5">[1] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, &ldquo;Labeled faces in the wild: A database for studying face recognition in unconstrained environments,&rdquo; in Workshop on faces in&rsquo;Real-Life&rsquo;Images: detection, alignment,</span></p><p class="c3"><span class="c8 c5">and recognition, 2008.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[2] F. Schroff, D. Kalenichenko, and J. Philbin, &ldquo;Facenet: A unified embedding for face recognition and clustering,&rdquo; in Proceedings of the IEEE conference on computer vision and pattern recognition , 2015, pp. 815&ndash;823.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[3] N. Kumar, A. Berg, P. N. Belhumeur, and S. Nayar, &ldquo;Describable visual attributes for face verification and image search,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 1962&ndash;1977, 2011.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, &ldquo;Imagenet: A large-scale hierarchical image database,&rdquo; in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. Ieee, 2009, pp. 248&ndash;255.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[5] M. Johnson and A. Savakis, &ldquo;L1-grassmann manifolds for robust face recognition,&rdquo; in Image Processing (ICIP), 2015 IEEE International Conference on. IEEE, 2015, pp. 482&ndash;486.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c31">[6] Imagesnippets. [Online]. Available: </span><span class="c52 c73 c31"><a class="c50" href="https://www.google.com/url?q=http://www.imagesnippets.com/&amp;sa=D&amp;ust=1544236211347000">www.imagesnippets.com/</a></span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[7] D. E. King, &ldquo;Dlib-ml: A machine learning toolkit,&rdquo; Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1755&ndash;1758, 2009.</span></p><p class="c3 c6"><span class="c5 c8"></span></p><p class="c3"><span class="c8 c5">[8] E. Learned-Miller, G. B. Huang, A. RoyChowdhury, H. Li, and G. Hua, &ldquo;Labeled faces in the wild: A survey,&rdquo; in Advances in face detection and facial image analysis. Springer, 2016, pp. 189&ndash;248.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">[9] D. Silverstein and J. Farrell, &ldquo;The relationship between image fidelity and image quality,&rdquo; Proceedings of 3rd IEEE International Conference on Image Processing.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3 c6"><span class="c12 c5"></span></p><p class="c55 c6"><span class="c0"></span></p><p class="c3"><span class="c40">X. Notes</span></p><p class="c3 c6"><span class="c0"></span></p><p class="c3"><span class="c8 c5">Model accuracy values are more specifically model accuracy on the LFW dataset. Obviously we cannot test the model on all possible images.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">Both LFW and LFW Attributes contain ground truths for the ontology to use for computing accuracy.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">Misclassifications associated with Occlusions</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 333.33px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 333.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8 c5">This is an example hierarchy view of the various forms of occlusion that occured in a test set. In this view, the user is given the various forms of expected occlusion organized into an &ldquo;is-a&rdquo; based tree. In the above diagram, the numbers in the circles are the percentage of misclassifications that have occured on an image that has been tagged with that attribute compared to the total tagged for that attribute. For example, exactly 47% of the images that are tagged as contained lipstick were correctly classified. Thus a person could assume that this was a major issue and it should be addressed, aka, receive a score of, &ldquo;bad.&rdquo; Only 82% of bald faces were correctly classified when the average for all forms of hair is 99, thus the model has an issue with bald people. We are currently considering adding inference rules for stings like statistical significance, bias, etc based on the essentials of research statistics.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c5 c15">Example Inferencing rules in Blazegraph</span></p><p class="c3"><span class="c52 c31 c73"><a class="c50" href="https://www.google.com/url?q=https://wiki.blazegraph.com/wiki/index.php/InferenceAndTruthMaintenance&amp;sa=D&amp;ust=1544236211350000">https://wiki.blazegraph.com/wiki/index.php/InferenceAndTruthMaintenance</a></span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c31">RDFS10 is: (?u,rdfs:subClassOf,?u) &lt;= (?u,rdf:type,rdfs:Class).<br><br></span><span class="c21">public class RuleRdfs10 extends Rule {<br> &nbsp; &nbsp;private static final long serialVersionUID = -2964784545354974663L;<br> &nbsp; &nbsp;public RuleRdfs10(String relationName, Vocabulary vocab) {<br> &nbsp; &nbsp; &nbsp; &nbsp;super( &nbsp;&quot;rdfs10&quot;,<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate(relationName,var(&quot;u&quot;), vocab.getConstant(RDFS.SUBCLASSOF), var(&quot;u&quot;)),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate[]{<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new SPOPredicate(relationName,var(&quot;u&quot;), vocab.getConstant(RDF.TYPE), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vocab.getConstant(RDFS.CLASS))},<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;null // constraints<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;);<br> &nbsp; &nbsp;} &nbsp; &nbsp;<br>}<br><br></span><span class="c8 c5">Once you have written your own rule, you need to incorporate it into one of the &quot;inference programs&quot;, typically the FullClosure program.</span></p><p class="c3 c6"><span class="c8 c5"></span></p><p class="c3"><span class="c8 c5">For example we could easily use this API to add additional rules such as:</span></p><p class="c3 c49"><span class="c31 c42">Correct Classification</span><span class="c8 c5">&nbsp;by querying for Result classes that have a prov:value that matches to who the Image is prov:attributedTo</span></p><p class="c3 c49 c6"><span class="c8 c5"></span></p><p class="c3 c49"><span class="c31 c42">Accuracy Ratings</span><span class="c8 c5">&nbsp;by querying for accuracy and asserting a label {good, bad, okay} if were within some range </span></p><p class="c3 c49 c6"><span class="c8 c5"></span></p><p class="c3 c49"><span class="c31">We haven&#39;t had a chance to implement any of these additional rules yet, but plan to in the near future.</span></p></body></html>